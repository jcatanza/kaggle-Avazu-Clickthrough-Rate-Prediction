{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with sklearn\n",
    "### ref: https://www.kaggle.com/c/avazu-ctr-prediction/discussion/12314"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Shuffle the training data, in the ``bash`` shell: \n",
    "### -- strip the header: ``tail -40429867 train.csv > train.csv``\n",
    "### -- shuffle the rows, in place: ``shuf -o train.csv < train.csv``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model, tune the hyperparameters by cross-validation, archive the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning hyper-parameters\n",
      "\n",
      "\n",
      "Best parameters set found on development set\n",
      "\n",
      "{'learning_rate': 0.03, 'max_depth': -1, 'min_data_in_leaf': 30, 'min_data_per_group': 5, 'num_leaves': 100, 'regression_l2': 0.0}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "-0.401 (+/-0.001) for {'learning_rate': 0.03, 'max_depth': -1, 'min_data_in_leaf': 30, 'min_data_per_group': 5, 'num_leaves': 100, 'regression_l2': 0.0}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set 90.0 % of the training samples\n",
      "The scores are computed on the full evaluation set 9.999999999999998 % of the training samples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcat\\Anaconda3\\envs\\xgboost\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.99      0.91     24913\n",
      "          1       0.62      0.07      0.13      5087\n",
      "\n",
      "avg / total       0.80      0.84      0.78     30000\n",
      "\n",
      "elapsed time: 0:04:07.954182, chunk: 1, log_loss: 0.398216\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "======================================================\n",
    "Out-of-core classification of  Avazu data\n",
    "======================================================\n",
    "wc count for train.csv 40428968\n",
    "wc count for test.csv   4577465\n",
    "\n",
    "Feature engineering\n",
    "Feature hashing\n",
    "SGD classifier with partial fit\n",
    "invscaling with eta0 =4-8\n",
    "l2 penalty\n",
    "labels scaled and shifted to -1,1 as vowpal wabbit\n",
    "\"\"\"\n",
    "\n",
    "# Authors: Elena Cuoco <elena.cuoco@gmail.com>\n",
    "\n",
    "# Avazu competitition using pandas and sklearn library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, time\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn import preprocessing \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import XGBClassifier #note: activate the xgboost environment\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# create matrix of hashed features and target labels vector\n",
    "\n",
    "\n",
    "# set the number of hash bins\n",
    "# 2**29 gets same result as 2**27, 2**30 produces memory error\n",
    "# n_features = 2**27\n",
    "n_features = 2**16\n",
    "\n",
    "\n",
    "# build a preprocessing pipeline for feature hashing\n",
    "preproc = Pipeline([('fh',FeatureHasher( n_features=n_features,input_type='string', non_negative=False))])\n",
    "\n",
    "def hash_features(data):\n",
    "    \n",
    "    # shift and scale target values to [-1,1] instead of [0,1]\n",
    "    # y_train = data['click'].values + data['click'].values-1\n",
    "    y_train = data['click'].values\n",
    "        \n",
    "    # remove id and click columns\n",
    "    data = data.drop(['id','click'], axis=1)\n",
    "    \n",
    "    # add engineered features related to datetime\n",
    "    add_engineered_datetime_features = True\n",
    "    if(add_engineered_datetime_features):    \n",
    "        data['hour']=data['hour'].map(lambda x: datetime.strptime(str(x),\"%y%m%d%H\"))\n",
    "        data['dayoftheweek']=data['hour'].map(lambda x:  x.weekday())\n",
    "        data['day']=data['hour'].map(lambda x:  x.day)\n",
    "        data['hour']=data['hour'].map(lambda x:  x.hour)\n",
    "        \n",
    "    # convert all features to str\n",
    "    X_train = np.asarray(data.astype(str))\n",
    "    \n",
    "    # hash the features\n",
    "    X_train = preproc.fit_transform(X_train)\n",
    "    \n",
    "    # target labels\n",
    "    y_train = np.asarray(y_train).ravel()\n",
    "    \n",
    "    # return y_train,X_train\n",
    "    # return y_train, X_train, data\n",
    "    return y_train, X_train\n",
    "\n",
    "\n",
    "###################################################################################\n",
    "# build and train model\n",
    "\n",
    "\n",
    "# set start time\n",
    "start = datetime.now()\n",
    "\n",
    "# file and folder paths\n",
    "train_file = 'train_shuffled.csv'\n",
    "model_path = './'\n",
    "\n",
    "# chunk_size\n",
    "chunk_size= 3e5\n",
    "\n",
    "# positive class fraction\n",
    "# positive_class_fraction = 0.17\n",
    "\n",
    "# number of training examples\n",
    "n_rows = chunk_size\n",
    "\n",
    "# seed for random generator\n",
    "seed = 99\n",
    "\n",
    "# option to archive model\n",
    "archive_model = False\n",
    "\n",
    "# column names\n",
    "header = ['id','click','hour','C1','banner_pos','site_id','site_domain','site_category','app_id','app_domain','app_category','device_id',\\\n",
    "        'device_ip','device_model','device_type','device_conn_type','C14','C15','C16','C17','C18','C19','C20','C21']\n",
    "\n",
    "\n",
    "# list of regularization hyperparameter values\n",
    "# C_range = [6, 3, 1]\n",
    "C_range = [1]\n",
    "\n",
    "# select classifier\n",
    "classifier = 'lgb'\n",
    "\n",
    "# loop over hyperparameter values\n",
    "for hyperparameter in C_range:\n",
    "\n",
    "    # build classifier\n",
    "        \n",
    "    if classifier == 'log':\n",
    "\n",
    "        # 'sag' solver is recommended for large data sets, does L2 but not L1 regularization\n",
    "        model = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=hyperparameter, \\\n",
    "            fit_intercept=True, intercept_scaling=1, class_weight=None, \\\n",
    "            random_state=seed, solver='sag', max_iter=300, multi_class='ovr', \\\n",
    "            verbose=0, warm_start=False, n_jobs=-1)\n",
    "        \n",
    "    elif classifier == 'sgd':\n",
    "        model = SGDClassifier(loss='log', tol=1.e-3, alpha=.0000001, penalty='l2',\\\n",
    "           shuffle=False,n_jobs=-1,random_state=seed,average=True,\\\n",
    "                          learning_rate='invscaling',power_t=0.5,eta0=4.0)\n",
    "\n",
    "    elif classifier == 'xgb':\n",
    "        params = {'learning_rate': 0.1, 'n_estimators': 100, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic', 'max_depth': 3, 'min_child_weight': 1, 'tree_method': 'hist'}\n",
    "        model = XGBClassifier(**params)\n",
    "\n",
    "    elif classifier == 'lgb':\n",
    "        \n",
    "        # set parameters\n",
    "        params = {}\n",
    "        params['boosting_type'] = 'gbdt'\n",
    "        params['objective'] = 'binary'\n",
    "        params['metric'] = 'binary_logloss'\n",
    "        # params['num_iterations'] = 100 # default\n",
    "        params['feature_fraction'] = 1.\n",
    "        params['bagging_fraction'] = 1.\n",
    "        params['nthreads'] = 8\n",
    "        # params['scale_pos_weight'] = 1 #positive_class_fraction\n",
    "        params['is_unbalance'] = False\n",
    "        params['max_bin'] = 2^12\n",
    "        params['n_estimators'] = 300\n",
    "        #params['categorical_feature'] = ['hour','dayoftheweek','day','C1','banner_pos','site_id','site_domain','site_category','app_id','app_domain','app_category','device_id',\\\n",
    "        #'device_ip','device_model','device_type','device_conn_type','C14','C15','C16','C17','C18','C19','C20','C21']\n",
    "        \n",
    "        # parameter grid to use with cross-validation\n",
    "        param_grid = {}\n",
    "        param_grid['min_data_in_leaf'] = [30] \n",
    "        param_grid['max_depth'] = [-1] \n",
    "        param_grid['learning_rate'] = [0.03]\n",
    "        param_grid['min_data_per_group'] = [5]\n",
    "        param_grid['num_leaves'] = [100] # <= 2**max_depth\n",
    "        param_grid['regression_l2'] = [0.]\n",
    "       \n",
    "        # build model\n",
    "        model = LGBMClassifier(**params)\n",
    "    \n",
    "    else:\n",
    "        raise Exception('Must specify classifier!')\n",
    "\n",
    "\n",
    "    # train classifier: iterate over batches\n",
    "    \n",
    "    # initialize\n",
    "    chunk=0\n",
    "    \n",
    "    # set up reader object for data input\n",
    "    # input data file has previously been randomly shuffled\n",
    "    reader = pd.read_table(train_file, sep=',', chunksize=chunk_size, names=header,header=None,\\\n",
    "           nrows = n_rows)\n",
    "\n",
    "    for data in reader:\n",
    "\n",
    "        # increment chunk counter\n",
    "        chunk+=1\n",
    "\n",
    "        # get next batch and preprocess\n",
    "        #y_train, X_train, data = hash_features(data)\n",
    "        y_train, X_train = hash_features(data)\n",
    "        \n",
    "        # data = data.apply(lambda x: x.astype('category'))\n",
    "        \n",
    "        # get positive class fraction\n",
    "        # positive_class_fraction = sum(y_train==1)/len(y_train)\n",
    "        \n",
    "        # fraction of data to use for training -- rest is for evaluation\n",
    "        train_fraction = 0.9\n",
    "        \n",
    "        # dev set for cross-validation: first train_fraction of training set\n",
    "        y_dev = y_train[:int(train_fraction*n_rows)]\n",
    "        X_dev = X_train[:int(train_fraction*n_rows),:]\n",
    "        #X_dev = data.iloc[:int(train_fraction*n_rows),:]\n",
    "    \n",
    "        # test set for model evaluation: remaining 1 - train_fraction of training set\n",
    "        y_test = y_train[int(train_fraction*n_rows):]\n",
    "        X_test = X_train[int(train_fraction*n_rows):,:]\n",
    "        # X_test = data.iloc[int(train_fraction*n_rows):,:]\n",
    "        \n",
    "        # Tune the hyperparameters by cross-validation\n",
    "        \n",
    "        print('Tuning hyper-parameters\\n\\n')\n",
    "        clf = GridSearchCV(model, param_grid=param_grid, cv=5,\n",
    "                           scoring='neg_log_loss')\n",
    "        clf.fit(X_dev, y_dev)\n",
    "\n",
    "        print('Best parameters set found on development set\\n')\n",
    "        print(clf.best_params_)\n",
    "        print('\\nGrid scores on development set:\\n')\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                  % (mean, std * 2, params))\n",
    "\n",
    "        print('\\nDetailed classification report:\\n')\n",
    "        print('The model is trained on the full development set {} % of the training samples'.format(100*train_fraction))\n",
    "        print('The scores are computed on the full evaluation set {} % of the training samples\\n'.format(100*(1-train_fraction)))\n",
    "        y_true, y_pred = y_test, clf.predict(X_test)\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        \n",
    "        # predict targets\n",
    "        y_prob = clf.best_estimator_.predict_proba(X_test)\n",
    "\n",
    "        # estimate log_loss\n",
    "        logloss = log_loss((y_test+1), y_prob)\n",
    "    \n",
    "        # Show progress\n",
    "        print('elapsed time: %s, chunk: %d, log_loss: %f' % (str(datetime.now() - start), chunk, logloss))\n",
    "    \n",
    "        # archive the model file from training\n",
    "        # !!!!! caution: model_file may be ~ GBs\n",
    "        archive_model = True\n",
    "        if(archive_model):\n",
    "            model_file = model_path+'model-avazu-sgd.pkl'\n",
    "            joblib.dump(clf.best_estimator_, model_file)\n",
    "\n",
    "            # archive the preprocessing file from training\n",
    "            preproc_file=model_path+'model-avazu-preproc.pkl'\n",
    "            joblib.dump(preproc, preproc_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
