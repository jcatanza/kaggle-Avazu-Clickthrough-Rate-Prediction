{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorization Machines with xLearn\n",
    "### ref:` https://xlearn-doc.readthedocs.io/en/latest/index.html`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlearn as xl\n",
    "\n",
    "# Training task\n",
    "ffm_model = xl.create_ffm() # Use field-aware factorization machine\n",
    "\n",
    "# On-disk training\n",
    "ffm_model.setOnDisk()\n",
    "\n",
    "ffm_model.setTrain(\"./small_train.txt\")  # Training data\n",
    "ffm_model.setValidate(\"./small_test.txt\")  # Validation data\n",
    "\n",
    "# param:\n",
    "#  0. binary classification\n",
    "#  1. learning rate: 0.2\n",
    "#  2. regular lambda: 0.002\n",
    "#  3. evaluation metric: accuracy\n",
    "param = {'task':'binary', 'lr':0.2,\n",
    "         'lambda':0.002, 'metric':'acc'}\n",
    "\n",
    "# Start to train\n",
    "# The trained model will be stored in model.out\n",
    "ffm_model.fit(param, './model.out')\n",
    "\n",
    "# Prediction task\n",
    "ffm_model.setTest(\"./small_test.txt\")  # Test data\n",
    "ffm_model.setSigmoid()  # Convert output to 0-1\n",
    "\n",
    "# Start to predict\n",
    "# The output result will be stored in output.txt\n",
    "ffm_model.predict(\"./model.out\", \"./output.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-96fbeb9a822b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# build a preprocessing pipeline for feature hashing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mpreproc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fh'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mFeatureHasher\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'string'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_negative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtransform_categorical_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcategorical_method\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdrop_column\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "XLearnLibraryNotFound",
     "evalue": "Cannot find xlearn Library in the candidate path",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXLearnLibraryNotFound\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cf038e9da4f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxlearn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxl\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Avazu competitition using pandas and sklearn library\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\xgboost\\lib\\site-packages\\xlearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mxlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mVERSION_FILE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VERSION'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\xgboost\\lib\\site-packages\\xlearn\\xlearn.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_LIB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXLearnHandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_check_call\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_str\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\xgboost\\lib\\site-packages\\xlearn\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# load the xlearn library globally\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0m_LIB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_lib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_check_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\xgboost\\lib\\site-packages\\xlearn\\base.py\u001b[0m in \u001b[0;36m_load_lib\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_load_lib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;34m\"\"\"Load xlearn shared library\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mlib_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_lib_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlib_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\xgboost\\lib\\site-packages\\xlearn\\libpath.py\u001b[0m in \u001b[0;36mfind_lib_path\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlib_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         raise XLearnLibraryNotFound(\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;34m'Cannot find xlearn Library in the candidate path'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m             )\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlib_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mXLearnLibraryNotFound\u001b[0m: Cannot find xlearn Library in the candidate path"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xlearn as xl\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Avazu competitition using pandas and sklearn library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import category_encoders as ce\n",
    "import xlearn as xl\n",
    "%matplotlib inline\n",
    "from datetime import datetime, date, time\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn import preprocessing \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import XGBClassifier #note: activate the xgboost environment\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "# method of handling categoricals: 'encode' or 'hash'\n",
    "categorical_method='encode'\n",
    "\n",
    "# build a preprocessing pipeline for feature hashing\n",
    "preproc = Pipeline([('fh',FeatureHasher( n_features=n_features,input_type='string', non_negative=False))])\n",
    "\n",
    "\n",
    "\n",
    "for data in pd.read_table(train_file, sep=',', chunksize=chunk_size, names=header,header=None,\\\n",
    "       nrows = n_rows):\n",
    "\n",
    "    # get next batch and preprocess\n",
    "    y_train, X_train, data = transform_categorical_features(data,categorical_method=categorical_method,drop_column=drop_column)\n",
    "\n",
    "X_train, X_val, y_train, y_val = \\\n",
    "    train_test_split(X_train, y_train, test_size=0.3, random_state=0)\n",
    "\n",
    "skip = True\n",
    "if(not skip):\n",
    "    # param:\n",
    "    #  0. binary classification\n",
    "    #  1. model scale: 0.1\n",
    "    #  2. epoch number: 10 (auto early-stop)\n",
    "    #  3. learning rate: 0.1\n",
    "    #  4. regular lambda: 1.0\n",
    "    #  5. use sgd optimization method\n",
    "    model = xl.LRModel(task='binary', init=0.1,\n",
    "                              epoch=10, lr=0.1,\n",
    "                              reg_lambda=1.0, opt='sgd')\n",
    "\n",
    "    # Start to train\n",
    "    model.fit(X_train, y_train,\n",
    "                     eval_set=[X_val, y_val],\n",
    "                     is_lock_free=False)\n",
    "\n",
    "    # Generate predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "\n",
    "# param:    # param:\n",
    "    #  0. binary classification\n",
    "    #  1. model scale: 0.1\n",
    "    #  2. epoch number: 10 (auto early-stop)\n",
    "    #  3. learning rate: 0.1\n",
    "    #  4. regular lambda: 1.0\n",
    "    #  5. use sgd optimization method\n",
    "    model = xl.LRModel(task='binary', init=0.1,\n",
    "                              epoch=10, lr=0.1,\n",
    "                              reg_lambda=1.0, opt='sgd')\n",
    "\n",
    "    # Start to train\n",
    "    model.fit(X_train, y_train,\n",
    "                     eval_set=[X_val, y_val],\n",
    "                     is_lock_free=False)\n",
    "\n",
    "    # Generate predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "#  0. binary classification\n",
    "#  1. learning rate: 0.2\n",
    "#  2. regular lambda: 0.002\n",
    "#  3. evaluation metric: accuracy\n",
    "param = {'task':'binary', 'lr':0.2,\n",
    "         'lambda':0.002, 'metric':'logloss'}\n",
    "\n",
    "# Start to train\n",
    "# The trained model will be stored in model.out\n",
    "ffm_model.fit(param, './FFM_model.out')\n",
    "\n",
    "# Prediction task\n",
    "ffm_model.setSigmoid()  # Convert output to 0-1\n",
    "\n",
    "# Start to predict\n",
    "# The output result will be stored in output.txt\n",
    "ffm_model.predict(\"./FFM_model.out\", \"./FFM_output.txt\")\n",
    "\n",
    "\n",
    "\n",
    "def transform_categorical_features(data,categorical_method,drop_column):\n",
    "    \n",
    "    # get click values\n",
    "    y_train = data['click'].values\n",
    "        \n",
    "    # target labels\n",
    "    y_train = np.asarray(y_train).ravel()\n",
    "        \n",
    "    # remove id and click columns\n",
    "    data = data.drop(['id','click'], axis=1)\n",
    "    print('start with ' +str(data.shape[1])+' features')\n",
    "    print(data.columns)\n",
    "    \n",
    "    # add engineered features related to datetime\n",
    "    add_engineered_datetime_features = True\n",
    "    if(add_engineered_datetime_features):    \n",
    "        data['hour']=data['hour'].map(lambda x: datetime.strptime(str(x),\"%y%m%d%H\"))\n",
    "        data['dayoftheweek']=data['hour'].map(lambda x:  x.weekday())\n",
    "        data['day']=data['hour'].map(lambda x:  x.day)\n",
    "        data['hour']=data['hour'].map(lambda x:  x.hour)\n",
    "        \n",
    "    # column names\n",
    "    # header = ['hour','C1','banner_pos','site_id','site_domain','site_category','app_id','app_domain','app_category','device_id',\\\n",
    "    #        'device_ip','device_model','device_type','device_conn_type','C14','C15','C16','C17','C18','C19','C20','C21']\n",
    "\n",
    "    # remove features\n",
    "    # data = data.drop(['C1'],axis=1)\n",
    "    # print(data.columns)\n",
    "    # drop_column = []\n",
    "    print('dropping '+str(data.columns[drop_column]))\n",
    "    data = data.drop(data.columns[drop_column], axis=1) \n",
    "    \n",
    "    \n",
    "    # hash or encode the features\n",
    "    if(categorical_method == 'hash'):\n",
    "        \n",
    "        # convert all features to str\n",
    "        X_train = np.asarray(data.astype(str))\n",
    "    \n",
    "        # hash all features\n",
    "        print('Hashing features...')\n",
    "        \n",
    "        # features\n",
    "        X_train = preproc.fit_transform(X_train)\n",
    "\n",
    "        \n",
    "    elif(categorical_method == 'encode'):\n",
    "        \n",
    "        # encode all features\n",
    "        print('Encoding features numerically...')\n",
    "        \n",
    "        # Encode columns\n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        data = data.apply(encoder.fit_transform)\n",
    "        \n",
    "        # one-hot encode -- fails due to memory error\n",
    "        # encoder = preprocessing.OneHotEncoder()\n",
    "        # encoder.fit(data)\n",
    "        # data = encoder.transform(data).toarray()\n",
    "        \n",
    "        print('There are ' +str(data.shape[1])+' features')\n",
    "        print(data.columns)\n",
    "        print(data.info())\n",
    "       \n",
    "        # binarize features\n",
    "        binarize=False\n",
    "        if(binarize):\n",
    "            print('Binarizing features...')\n",
    "\n",
    "\n",
    "            # From category_encoders\n",
    "            encoder = ce.BinaryEncoder(cols=data.columns.tolist()).fit(data)\n",
    "            data = encoder.transform(data)\n",
    "        \n",
    "        # features\n",
    "        X_train = np.asarray(data) \n",
    "        \n",
    "   \n",
    "    return y_train, X_train, data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:xgboost]",
   "language": "python",
   "name": "conda-env-xgboost-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
