{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fced38b6c2c8806594006ac26c990c88bdee2dcd"
   },
   "source": [
    "## kaggle Avazu project \n",
    "https://www.kaggle.com/c/avazu-ctr-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "a04f5cc9d1d3724190ff53cf71dfe610c1ca1e72"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-13-f78ef2bd24be>, line 69)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-f78ef2bd24be>\"\u001b[1;36m, line \u001b[1;32m69\u001b[0m\n\u001b[1;33m    else:\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# %load fast_solution.py\n",
    "'''\n",
    "           DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE\n",
    "                   Version 2, December 2004\n",
    "\n",
    "Copyright (C) 2004 Sam Hocevar <sam@hocevar.net>\n",
    "Modified by : Abhishek Thakur <abhishek4@gmail.com>\n",
    "\n",
    "Everyone is permitted to copy and distribute verbatim or modified\n",
    "copies of this license document, and changing it is allowed as long\n",
    "as the name is changed.\n",
    "\n",
    "           DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE\n",
    "  TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION\n",
    "\n",
    " 0. You just DO WHAT THE FUCK YOU WANT TO.\n",
    "'''\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from math import log, exp, sqrt\n",
    "\n",
    "\n",
    "# TL; DR\n",
    "# the main learning process start at line 122\n",
    "\n",
    "\n",
    "# parameters #################################################################\n",
    "\n",
    "#train = '../input/train.csv'  # path to training file\n",
    "#test = '../input/test.csv'  # path to testing file\n",
    "train = 'train.csv'  # path to training file\n",
    "test = 'test.csv'  # path to testing file\n",
    "\n",
    "D = 2 ** 20  # number of weights use for each model, we have 32 of them\n",
    "alpha = .1   # learning rate for sgd optimization\n",
    "\n",
    "\n",
    "# function, generator definitions ############################################\n",
    "\n",
    "# A. x, y generator\n",
    "# INPUT:\n",
    "#     path: path to train.csv or test.csv\n",
    "#     label_path: (optional) path to trainLabels.csv\n",
    "# YIELDS:\n",
    "#     ID: id of the instance (can also acts as instance count)\n",
    "#     x: a list of indices that its value is 1\n",
    "#     y: (if label_path is present) label value of y1 to y33\n",
    "def data(path, traindata=False):\n",
    "    for t, line in enumerate(open(path)):\n",
    "        # initialize our generator\n",
    "        if t == 0:\n",
    "            # create a static x,\n",
    "            # so we don't have to construct a new x for every instance\n",
    "            # x = [0] * 27\n",
    "            x = [0] * 24\n",
    "            # is continue statement necessary? Yes, to skip header line\n",
    "            # print(line.rstrip().split(','))\n",
    "            # Skip header line\n",
    "            continue\n",
    "        # parse x\n",
    "        for m, feat in enumerate(line.rstrip().split(',')):\n",
    "            if m == 0:\n",
    "                ID = int(feat)\n",
    "            elif traindata and m == 1:\n",
    "                y = [float(feat)]\n",
    "            elif not traindata and m == 1:\n",
    "                pass\n",
    "            else:\n",
    "                # one-hot encode everything with hash trick\n",
    "                # categorical: one-hotted\n",
    "                # boolean: ONE-HOTTED\n",
    "                # numerical: ONE-HOTTED!\n",
    "                # note, the build in hash(), although fast is not stable,\n",
    "                #       i.e., same value won't always have the same hash\n",
    "                #       on different machines\n",
    "                if traindata:\n",
    "                    x[m] = abs(hash(str(m) + '_' + feat)) % D\n",
    "                #else:\n",
    "                #    x[m+1] = abs(hash(str(m+1) + '_' + feat)) % D\n",
    "        # Why include x[0] and x[1]? Seems like we should ignore the first 2 entries of x, because they are both always 0.\n",
    "        # And why does x have 27 entries?\n",
    "        # yield (ID, x, y) if traindata else (ID, x)\n",
    "        features = x[2:]\n",
    "        yield (ID, features, y) if traindata else (ID, features)\n",
    "\n",
    "# B. Bounded logloss\n",
    "# INPUT:\n",
    "#     p: our prediction\n",
    "#     y: real answer\n",
    "# OUTPUT\n",
    "#     bounded logarithmic loss of p given y\n",
    "def logloss(p, y):\n",
    "    p = max(min(p, 1. - 10e-15), 10e-15)\n",
    "    return -log(p) if y == 1. else -log(1. - p)\n",
    "\n",
    "\n",
    "# C. Get probability estimation on x\n",
    "# INPUT:\n",
    "#     x: features\n",
    "#     w: weights\n",
    "# OUTPUT:\n",
    "#     probability of p(y = 1 | x; w)\n",
    "def predict(x, w):\n",
    "    wTx = 0.\n",
    "    for i in x:  # do wTx\n",
    "        wTx += w[i] * 1.  # w[i] * x[i], but if i in x we got x[i] = 1.\n",
    "    return 1. / (1. + exp(-max(min(wTx, 20.), -20.)))  # bounded sigmoid\n",
    "\n",
    "\n",
    "# D. Update given model\n",
    "# INPUT:\n",
    "# alpha: learning rate\n",
    "#     w: weights\n",
    "#     n: sum of previous absolute gradients for a given feature\n",
    "#        this is used for adaptive learning rate\n",
    "#     x: feature, a list of indices\n",
    "#     p: prediction of our model\n",
    "#     y: answer\n",
    "# MODIFIES:\n",
    "#     w: weights\n",
    "#     n: sum of past absolute gradients\n",
    "def update(alpha, w, n, x, p, y):\n",
    "    for i in x:\n",
    "        # alpha / sqrt(n) is the adaptive learning rate\n",
    "        # (p - y) * x[i] is the current gradient\n",
    "        # note that in our case, if i in x then x[i] = 1.\n",
    "        n[i] += abs(p - y)\n",
    "        w[i] -= (p - y) * 1. * alpha / sqrt(n[i])\n",
    "\n",
    "\n",
    "# training and testing #######################################################\n",
    "start = datetime.now()\n",
    "\n",
    "K = [0]\n",
    "\n",
    "w = [[0.] * D]\n",
    "n = [[0.] * D]\n",
    "\n",
    "loss = 0.\n",
    "\n",
    "tt = 1\n",
    "for ID, x, y in data(train, traindata = True):\n",
    "\n",
    "    # get predictions and train on all labels\n",
    "    for k in K:\n",
    "        p = predict(x, w[k])\n",
    "        update(alpha, w[k], n[k], x, p, y[k])\n",
    "        loss += logloss(p, y[k])  # for progressive validation\n",
    "\n",
    "    # print out progress, so that we know everything is working\n",
    "    if tt % 100000 == 0:\n",
    "        print('%s\\tencountered: %d\\tcurrent logloss: %f' % (\n",
    "                datetime.now(), tt, (loss * 1./tt)))\n",
    "    tt += 1\n",
    "\n",
    "with open('submission.csv', 'w') as outfile:\n",
    "    outfile.write('id,click\\n')\n",
    "    for ID, x in data(test):\n",
    "        for k in K:\n",
    "            p = predict(x, w[k])\n",
    "            outfile.write('%s,%s\\n' % (ID, str(p)))\n",
    "\n",
    "print('Done, elapsed time: %s' % str(datetime.now() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aab7264f55d9ddd91853fc4bffbb8cba231650e0",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
